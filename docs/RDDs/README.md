# RDD编程指南

在应用层上，每个Spark应用程序都由一个驱动程序组成，该驱动程序运行用户的main函数（这里并不指程序main入口函数），并在集群上执行各种并行操作。

Spark提供的主要抽象是一个弹性分布式数据集(RDD)，它是跨集群节点分区的元素集合，可以并行操作。RDD是从Hadoop文件系统(或任何其他支持Hadoop的文件系统)中的文件或驱动程序中现有的Scala集合创建的，并对其执行transform操作。用户还可能要求Spark在内存中持久化RDD，从而允许在做并行操作时高效地重用RDD。最后，RDDs还可以自动从故障节点中恢复。

Spark中的第二个抽象是用于并行操作的`共享变量`。默认情况下，当Spark在不同节点上以任务集的形式并行运行函数时，函数中使用的每个变量的副本会被发送给每个任务。有时，变量需要在任务之间共享，或者在任务和驱动程序之间共享。Spark支持两种共享变量：广播变量（Broadcast Variables），可在所有节点上的内存中缓存；累加器（Accumulators），由Spark提供的类型，只能用来做加减法，比如计数器与求和。

在开始本教程之前，请先运行bin/pyspark，并跟着实践操作，会更容易理解它。